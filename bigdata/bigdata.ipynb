{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./anaconda3/lib/python3.7/site-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in ./anaconda3/lib/python3.7/site-packages (from pyspark) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspark\n",
    "#To install pyspark if it has not been installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start using Pyspark, we need to type some prerequisite code. Don't worry about what they mean for the moment. It will be the same for every Spark program we write, except for the argument in setAppName, which is a name you can give to your program. You should also only run them once in any Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"abc\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central object of Pyspark is a Resilient Distributed Dataset, or RDD. You can basically consider an RDD like a Python list. Each element in an RDD is conventionally called a \"line\". For example, we can simply create a four-line RDD by typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions that an RDD can perform can be divided into two main types: transformations and actions. Pyspark does not perform any calculation until an action is performed, to save computing power. For example, the <code>collect</code> function is an action that shows the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, I will be using <code>collect</code> a lot to explain what a step does. However, do keep in mind that in a real big data project, actions like <code>collect</code> should be used sparingly, because in big data you will want to avoid spending computing power as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously in a big data project, the data will come in a separate file, usually a csv file. So we will not be using the parallelize function much. In the following code we load data from a separate file into an RDD, then use the standard Python slicing function to show the first 5 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,Will,33,385',\n",
       " '1,Jean-Luc,26,2',\n",
       " '2,Hugh,55,221',\n",
       " '3,Deanna,40,465',\n",
       " '4,Quark,68,21']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"fakefriends.csv\")\n",
    "lines.collect()[0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now we want to extract some information from this RDD. This csv file is a dataset about the number of friends somebody's friends have. The first column is the index, the second is the friend's name, and the third the friend's age, and the fourth the number of friends that friend has. For example, the first friend Will is 33 years old and has 385 friends himself. </p>\n",
    "<p> Let's say we want to know the average number of friends a friend has, if that friend is a given age. Obviously, the names become irrelevant, so we want to first filter out any irrelevant information. To transform an RDD, we need to first write a Python function that transforms one line in an RDD. We see that each line is a string, and each string contains information separated by commas. That suggests using the split function to turn the string into a list of separate items: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 385)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(\",\")\n",
    "    age = int(fields[2])\n",
    "    num = int(fields[3])\n",
    "    return (age,num)\n",
    "\n",
    "parseLine(lines.collect()[0]) #Trying out our defined function on the first line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to apply that function to all lines in the RDD, and create a new RDD, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each line in the RDD is in the format (age,number of friends). This is a very common format of any big dataset called a <b>key-value pair</b>: the first element of the pair being the key and the second being the value. Now we want to add up all the values with the same key, then divide that sum by the number of times the key appears. One neat trick to doing that is to first transform the rdd into a new RDD of the format (age,(num,1)). Now since this is a much simpler operation, we shall utilize the lambda function in Python. The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, (385, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.mapValues(lambda x: (x,1))\n",
    "rdd.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that all transformation functions in Pyspark do not change an RDD in place, but rather create a new one. And in cases when we do not change the key in a key-value pair such as this one, we should use the <code>mapValues</code> function because it is faster. Now we can add up all the values with the same key in the RDD. Usually when we want to transform an RDD by grouping up all key-value pairs with the same key in some fashion, we use the <code>reduceByKey</code> function. The following code might be a little harder to understand, but in the end it produces a new RDD of key-value pairs, with the the first sum in the value being the total number of friends and the second the total number of occurrence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, (3904, 12))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalsByAge = rdd.reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "totalsByAge.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, for example, there are 12 friends aged 33 and the total number of friends all of them have is 3904. Therefore to find the average number of friends each age has is a matter of simply dividing the first number in the value to the second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 325.3333333333333),\n",
       " (26, 242.05882352941177),\n",
       " (55, 295.53846153846155),\n",
       " (40, 250.8235294117647),\n",
       " (68, 269.6),\n",
       " (59, 220.0),\n",
       " (37, 249.33333333333334),\n",
       " (54, 278.0769230769231),\n",
       " (38, 193.53333333333333),\n",
       " (27, 228.125),\n",
       " (53, 222.85714285714286),\n",
       " (57, 258.8333333333333),\n",
       " (56, 306.6666666666667),\n",
       " (43, 230.57142857142858),\n",
       " (36, 246.6),\n",
       " (22, 206.42857142857142),\n",
       " (35, 211.625),\n",
       " (45, 309.53846153846155),\n",
       " (60, 202.71428571428572),\n",
       " (67, 214.625),\n",
       " (19, 213.27272727272728),\n",
       " (30, 235.8181818181818),\n",
       " (51, 302.14285714285717),\n",
       " (25, 197.45454545454547),\n",
       " (21, 350.875),\n",
       " (42, 303.5),\n",
       " (49, 184.66666666666666),\n",
       " (48, 281.4),\n",
       " (50, 254.6),\n",
       " (39, 169.28571428571428),\n",
       " (32, 207.9090909090909),\n",
       " (58, 116.54545454545455),\n",
       " (64, 281.3333333333333),\n",
       " (31, 267.25),\n",
       " (52, 340.6363636363636),\n",
       " (24, 233.8),\n",
       " (20, 165.0),\n",
       " (62, 220.76923076923077),\n",
       " (41, 268.55555555555554),\n",
       " (44, 282.1666666666667),\n",
       " (69, 235.2),\n",
       " (65, 298.2),\n",
       " (61, 256.22222222222223),\n",
       " (28, 209.1),\n",
       " (66, 276.44444444444446),\n",
       " (46, 223.69230769230768),\n",
       " (29, 215.91666666666666),\n",
       " (18, 343.375),\n",
       " (47, 233.22222222222223),\n",
       " (34, 245.5),\n",
       " (63, 384.0),\n",
       " (23, 246.3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByAge = totalsByAge.mapValues(lambda x: x[0]/x[1])\n",
    "averageByAge.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the final result we are looking for. It does not mean much as it is only a fake dataset randomly generated, but I hope it gives a good introduction to the general ideas of RDDs and a few commonly used functions. Putting all the different steps together in a complete program, minus the collect calls in the intermediate steps, may looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=abc, master=local) created by __init__ at <ipython-input-2-f696d91b152a>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f4d5b3b4e977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FriendsByAge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparseLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=abc, master=local) created by __init__ at <ipython-input-2-f696d91b152a>:3 "
     ]
    }
   ],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "lines = sc.textFile(\"fakefriends.csv\")\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "results = averagesByAge.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
