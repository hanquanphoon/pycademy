{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ml-100k/u.data is a file that contains 100 thousand movie ratings. The first column is the ID of a user, the second the movie ID, the third the rating, and the fourth the date. Let's say we want to extract some information from the dataset. Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Movie\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"u.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start, let's say we want to know what is the most rated movies. Following the examples in the last chapter, the code should looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"u.data\")\n",
    "movies = lines.map(lambda x: (int(x.split()[1]), 1))\n",
    "movieCounts = movies.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "flipped = movieCounts.map( lambda xy: (xy[1],xy[0]) )\n",
    "sortedMovies = flipped.sortByKey(ascending=False)\n",
    "\n",
    "results = sortedMovies.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the result gives you the number of ratings a movie receives, but it only identifies the movie by a number, which is less meaningful. In the file \"u.item\" there are movie names that correspond to the movie ID in other files. We can use the <code>braodcast</code> function in Pyspark to show the movie names instead of movie ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [[2, 3], 4])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.ITEM\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "nameDict = sc.broadcast(loadMovieNames())\n",
    "\n",
    "lines = sc.textFile(\"file:///SparkCourse/ml-100k/u.data\")\n",
    "movies = lines.map(lambda x: (int(x.split()[1]), 1))\n",
    "movieCounts = movies.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "flipped = movieCounts.map( lambda x : (x[1], x[0]))\n",
    "sortedMovies = flipped.sortByKey()\n",
    "\n",
    "sortedMoviesWithNames = sortedMovies.map(lambda countMovie : (nameDict.value[countMovie[1]], countMovie[0]))\n",
    "\n",
    "results = sortedMoviesWithNames.collect()\n",
    "\n",
    "for result in results:\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Marvel-graph\" file contains a list of superheroes from the Marvel universe, identified using a number, and the other superheroes who appear in the same work as them. Some superhero may appear more than once in the first column, because they appear with so many other superheroes that the list has to be broken into multiple lines. If we want to know who the most popular superhero is, that is the superhero who appears in the same work with the highest number of other superheroes, the code in Pyspark may look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCoOccurences(line):\n",
    "    elements = line.split()\n",
    "    return (int(elements[0]), len(elements) - 1)\n",
    "\n",
    "def parseNames(line):\n",
    "    fields = line.split('\\\"')\n",
    "    return (int(fields[0]), fields[1].encode(\"utf8\"))\n",
    "\n",
    "names = sc.textFile(\"file:///SparkCourse/marvel-names.txt\")\n",
    "namesRdd = names.map(parseNames)\n",
    "\n",
    "lines = sc.textFile(\"file:///SparkCourse/marvel-graph.txt\")\n",
    "\n",
    "pairings = lines.map(countCoOccurences)\n",
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n",
    "flipped = totalFriendsByCharacter.map(lambda xy : (xy[1], xy[0]))\n",
    "\n",
    "mostPopular = flipped.max()\n",
    "\n",
    "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
    "\n",
    "print(str(mostPopularName) + \" is the most popular superhero, with \" + \\\n",
    "    str(mostPopular[0]) + \" co-appearances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say instead we are interested in knowing the degree of separation between two superheroes is. The code looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# The characters we wish to find the degree of separation between:\n",
    "startCharacterID = 5306 #SpiderMan\n",
    "targetCharacterID = 14  #ADAM 3,031 (who?)\n",
    "\n",
    "# Our accumulator, used to signal when we find the target character during\n",
    "# our BFS traversal.\n",
    "hitCounter = sc.accumulator(0)\n",
    "\n",
    "def convertToBFS(line):\n",
    "    fields = line.split()\n",
    "    heroID = int(fields[0])\n",
    "    connections = []\n",
    "    for connection in fields[1:]:\n",
    "        connections.append(int(connection))\n",
    "\n",
    "    color = 'WHITE'\n",
    "    distance = 9999\n",
    "\n",
    "    if (heroID == startCharacterID):\n",
    "        color = 'GRAY'\n",
    "        distance = 0\n",
    "\n",
    "    return (heroID, (connections, distance, color))\n",
    "\n",
    "\n",
    "def createStartingRdd():\n",
    "    inputFile = sc.textFile(\"file:///sparkcourse/marvel-graph.txt\")\n",
    "    return inputFile.map(convertToBFS)\n",
    "\n",
    "def bfsMap(node):\n",
    "    characterID = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    color = data[2]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    #If this node needs to be expanded...\n",
    "    if (color == 'GRAY'):\n",
    "        for connection in connections:\n",
    "            newCharacterID = connection\n",
    "            newDistance = distance + 1\n",
    "            newColor = 'GRAY'\n",
    "            if (targetCharacterID == connection):\n",
    "                hitCounter.add(1)\n",
    "\n",
    "            newEntry = (newCharacterID, ([], newDistance, newColor))\n",
    "            results.append(newEntry)\n",
    "\n",
    "        #We've processed this node, so color it black\n",
    "        color = 'BLACK'\n",
    "\n",
    "    #Emit the input node so we don't lose it.\n",
    "    results.append( (characterID, (connections, distance, color)) )\n",
    "    return results\n",
    "\n",
    "def bfsReduce(data1, data2):\n",
    "    edges1 = data1[0]\n",
    "    edges2 = data2[0]\n",
    "    distance1 = data1[1]\n",
    "    distance2 = data2[1]\n",
    "    color1 = data1[2]\n",
    "    color2 = data2[2]\n",
    "\n",
    "    distance = 9999\n",
    "    color = color1\n",
    "    edges = []\n",
    "\n",
    "    # See if one is the original node with its connections.\n",
    "    # If so preserve them.\n",
    "    if (len(edges1) > 0):\n",
    "        edges.extend(edges1)\n",
    "    if (len(edges2) > 0):\n",
    "        edges.extend(edges2)\n",
    "\n",
    "    # Preserve minimum distance\n",
    "    if (distance1 < distance):\n",
    "        distance = distance1\n",
    "\n",
    "    if (distance2 < distance):\n",
    "        distance = distance2\n",
    "\n",
    "    # Preserve darkest color\n",
    "    if (color1 == 'WHITE' and (color2 == 'GRAY' or color2 == 'BLACK')):\n",
    "        color = color2\n",
    "\n",
    "    if (color1 == 'GRAY' and color2 == 'BLACK'):\n",
    "        color = color2\n",
    "\n",
    "    if (color2 == 'WHITE' and (color1 == 'GRAY' or color1 == 'BLACK')):\n",
    "        color = color1\n",
    "\n",
    "    if (color2 == 'GRAY' and color1 == 'BLACK'):\n",
    "        color = color1\n",
    "\n",
    "    return (edges, distance, color)\n",
    "\n",
    "\n",
    "#Main program here:\n",
    "iterationRdd = createStartingRdd()\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    print(\"Running BFS iteration# \" + str(iteration+1))\n",
    "\n",
    "    # Create new vertices as needed to darken or reduce distances in the\n",
    "    # reduce stage. If we encounter the node we're looking for as a GRAY\n",
    "    # node, increment our accumulator to signal that we're done.\n",
    "    mapped = iterationRdd.flatMap(bfsMap)\n",
    "\n",
    "    # Note that mapped.count() action here forces the RDD to be evaluated, and\n",
    "    # that's the only reason our accumulator is actually updated.\n",
    "    print(\"Processing \" + str(mapped.count()) + \" values.\")\n",
    "\n",
    "    if (hitCounter.value > 0):\n",
    "        print(\"Hit the target character! From \" + str(hitCounter.value) \\\n",
    "            + \" different direction(s).\")\n",
    "        break\n",
    "\n",
    "    # Reducer combines data for each character ID, preserving the darkest\n",
    "    # color and shortest path.\n",
    "    iterationRdd = mapped.reduceByKey(bfsReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you shop online, in the shopping website you may be recommended certain items you might be interested in. Or when you are on Netflix, you may be recommended other movies or tv shows. One of the ways the websites can figure out what you might like is through the following thinking. In the 100 thousand movie ratings file, there are many users who rated more than one movie. If a user rated two movies similarly, say within one star difference, we might reasonably guess that the two movies are similar, or at least the other user who likes one of the two movies will like the other as well. Using this line of thinking, the code that ranks all the movies to any particular user may look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from math import sqrt\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.ITEM\", encoding='ascii', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "#Python 3 doesn't let you pass around unpacked tuples,\n",
    "#so we explicitly extract the ratings now.\n",
    "def makePairs( userRatings ):\n",
    "    ratings = userRatings[1]\n",
    "    (movie1, rating1) = ratings[0]\n",
    "    (movie2, rating2) = ratings[1]\n",
    "    return ((movie1, movie2), (rating1, rating2))\n",
    "\n",
    "def filterDuplicates( userRatings ):\n",
    "    ratings = userRatings[1]\n",
    "    (movie1, rating1) = ratings[0]\n",
    "    (movie2, rating2) = ratings[1]\n",
    "    return movie1 < movie2\n",
    "\n",
    "def computeCosineSimilarity(ratingPairs):\n",
    "    numPairs = 0\n",
    "    sum_xx = sum_yy = sum_xy = 0\n",
    "    for ratingX, ratingY in ratingPairs:\n",
    "        sum_xx += ratingX * ratingX\n",
    "        sum_yy += ratingY * ratingY\n",
    "        sum_xy += ratingX * ratingY\n",
    "        numPairs += 1\n",
    "\n",
    "    numerator = sum_xy\n",
    "    denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
    "\n",
    "    score = 0\n",
    "    if (denominator):\n",
    "        score = (numerator / (float(denominator)))\n",
    "\n",
    "    return (score, numPairs)\n",
    "\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MovieSimilarities\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "print(\"\\nLoading movie names...\")\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "data = sc.textFile(\"file:///SparkCourse/ml-100k/u.data\")\n",
    "\n",
    "# Map ratings to key / value pairs: user ID => movie ID, rating\n",
    "ratings = data.map(lambda l: l.split()).map(lambda l: (int(l[0]), (int(l[1]), float(l[2]))))\n",
    "\n",
    "# Emit every movie rated together by the same user.\n",
    "# Self-join to find every combination.\n",
    "joinedRatings = ratings.join(ratings)\n",
    "\n",
    "# At this point our RDD consists of userID => ((movieID, rating), (movieID, rating))\n",
    "\n",
    "# Filter out duplicate pairs\n",
    "uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)\n",
    "\n",
    "# Now key by (movie1, movie2) pairs.\n",
    "moviePairs = uniqueJoinedRatings.map(makePairs)\n",
    "\n",
    "# We now have (movie1, movie2) => (rating1, rating2)\n",
    "# Now collect all ratings for each movie pair and compute similarity\n",
    "moviePairRatings = moviePairs.groupByKey()\n",
    "\n",
    "# We now have (movie1, movie2) = > (rating1, rating2), (rating1, rating2) ...\n",
    "# Can now compute similarities.\n",
    "moviePairSimilarities = moviePairRatings.mapValues(computeCosineSimilarity).cache()\n",
    "\n",
    "# Save the results if desired\n",
    "#moviePairSimilarities.sortByKey()\n",
    "#moviePairSimilarities.saveAsTextFile(\"movie-sims\")\n",
    "\n",
    "# Extract similarities for the movie we care about that are \"good\".\n",
    "if (len(sys.argv) > 1):\n",
    "\n",
    "    scoreThreshold = 0.97\n",
    "    coOccurenceThreshold = 50\n",
    "\n",
    "    movieID = int(sys.argv[1])\n",
    "\n",
    "    # Filter for movies with this sim that are \"good\" as defined by\n",
    "    # our quality thresholds above\n",
    "    filteredResults = moviePairSimilarities.filter(lambda pairSim: \\\n",
    "        (pairSim[0][0] == movieID or pairSim[0][1] == movieID) \\\n",
    "        and pairSim[1][0] > scoreThreshold and pairSim[1][1] > coOccurenceThreshold)\n",
    "\n",
    "    # Sort by quality score.\n",
    "    results = filteredResults.map(lambda pairSim: (pairSim[1], pairSim[0])).sortByKey(ascending = False).take(10)\n",
    "\n",
    "    print(\"Top 10 similar movies for \" + nameDict[movieID])\n",
    "    for result in results:\n",
    "        (sim, pair) = result\n",
    "        # Display the similarity result that isn't the movie we're looking at\n",
    "        similarMovieID = pair[0]\n",
    "        if (similarMovieID == movieID):\n",
    "            similarMovieID = pair[1]\n",
    "        print(nameDict[similarMovieID] + \"\\tscore: \" + str(sim[0]) + \"\\tstrength: \" + str(sim[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways you can improve your algorithms. For example, you may want to exclude movies that receives poor ratings from your recommendation list. Or only consider the instance of a user liking two movies similarly and not disliking two movies similarly. Or rank movies according to similarity with another movie, instead of recommend to a user. Pick one, think of how to change your program and implement it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
